{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 7681928,
          "sourceType": "datasetVersion",
          "datasetId": 9
        },
        {
          "sourceId": 11371,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5171
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "[Gemma] I am replacing myself with an LLM",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carolina-Gpa/ml-experiments-template/blob/main/%5BGemma%5D_I_am_replacing_myself_with_an_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***If you can't beat 'em, join 'em...***\n",
        "**Author: [Carl McBride Ellis](https://www.kaggle.com/carlmcbrideellis)** ([LinkedIn](https://www.linkedin.com/in/carl-mcbride-ellis/))\n",
        "\n",
        "Looking at the state of the Kaggle forums of late, with [an explosion of AI Generated Text](https://www.kaggle.com/discussions/general/398579), I think the time has come to replace myself with the [Gemma LLM](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf) and from now on dedicate my time to more worthwhile endeavors.\n",
        "The quality of my replies will now be even worse than usual, and I will receive fewer medals, but I was never in it for the medals in the first place.\n",
        "\n",
        "Here we shall fine tune the [Gemma](https://www.kaggle.com/models/google/gemma) model with my historical replies scraped from the [Meta Kaggle](https://www.kaggle.com/datasets/kaggle/meta-kaggle) dataset.\n",
        "This work is extremely heavily based of the following two magnificent notebooks by [Nilay Chauhan](https://www.kaggle.com/nilaychauhan)\n",
        "* [Get started with Gemma using KerasNLP](https://www.kaggle.com/code/nilaychauhan/get-started-with-gemma-using-kerasnlp)\n",
        "* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "bPtFjiyeo3kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 256)\n",
        "import re\n",
        "# from https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora\n",
        "\n",
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n",
        "\n",
        "import keras\n",
        "import keras_nlp"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-02-22T18:57:56.534291Z",
          "iopub.execute_input": "2024-02-22T18:57:56.534642Z",
          "iopub.status.idle": "2024-02-22T18:58:37.465644Z",
          "shell.execute_reply.started": "2024-02-22T18:57:56.534613Z",
          "shell.execute_reply": "2024-02-22T18:58:37.464757Z"
        },
        "trusted": true,
        "id": "XzlT2jaUo3ke",
        "outputId": "ef577fe2-d19c-4dca-a3a1-c2c182fbc606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/465.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/465.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:58:37.467311Z",
          "iopub.execute_input": "2024-02-22T18:58:37.467832Z",
          "iopub.status.idle": "2024-02-22T18:59:39.514743Z",
          "shell.execute_reply.started": "2024-02-22T18:58:37.467805Z",
          "shell.execute_reply": "2024-02-22T18:59:39.513745Z"
        },
        "trusted": true,
        "id": "B6vSdA55o3kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before..."
      ],
      "metadata": {
        "id": "VhRSrqZgo3kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gemma_lm.generate(\"Will there be a private leaderboard shakeup?\", max_length=256))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:39.516024Z",
          "iopub.execute_input": "2024-02-22T18:59:39.516403Z",
          "iopub.status.idle": "2024-02-22T18:59:56.641157Z",
          "shell.execute_reply.started": "2024-02-22T18:59:39.51637Z",
          "shell.execute_reply": "2024-02-22T18:59:56.640156Z"
        },
        "trusted": true,
        "id": "CVLBcfeZo3kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataset of my Kaggle conversations obtained from [Meta Kaggle](https://www.kaggle.com/datasets/kaggle/meta-kaggle)"
      ],
      "metadata": {
        "id": "UBdby6hIo3kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get my Kaggle User Id\n",
        "Users = pd.read_csv(\"/kaggle/input/meta-kaggle/Users.csv\")\n",
        "User_Id = Users.query('UserName == \"carlmcbrideellis\"')[\"Id\"].item()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.338882Z",
          "iopub.status.idle": "2024-02-22T18:59:57.339387Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.339142Z",
          "shell.execute_reply": "2024-02-22T18:59:57.339163Z"
        },
        "trusted": true,
        "id": "p9BYDMuZo3kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ForumMessages = pd.read_csv(\"/kaggle/input/meta-kaggle/ForumMessages.csv\")\n",
        "CME_posts = ForumMessages[(ForumMessages['PostUserId'] == User_Id)] #[\"Message\"]\n",
        "# Select only my posts that were replies\n",
        "CME_Response = CME_posts[CME_posts['ReplyToForumMessageId'].notna()]\n",
        "CME_Response = CME_Response.rename(columns={'Message': 'Response'})\n",
        "# and now get the correponding \"prompt\"\n",
        "CME_Response = CME_Response.merge(ForumMessages, left_on='ReplyToForumMessageId', right_on='Id')\n",
        "CME_Response = CME_Response.rename(columns={'Message': 'Instruction'})\n",
        "data = CME_Response[[\"Instruction\",\"Response\"]].copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.340847Z",
          "iopub.status.idle": "2024-02-22T18:59:57.341178Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.341013Z",
          "shell.execute_reply": "2024-02-22T18:59:57.341025Z"
        },
        "trusted": true,
        "id": "orfFVmtvo3kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "how many conversations are there?"
      ],
      "metadata": {
        "id": "sIRzLEUYo3kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.342204Z",
          "iopub.status.idle": "2024-02-22T18:59:57.342507Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.342354Z",
          "shell.execute_reply": "2024-02-22T18:59:57.342366Z"
        },
        "trusted": true,
        "id": "4iFS8Y3Mo3kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, almost 2 thousand"
      ],
      "metadata": {
        "id": "tFMUsIYdo3kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do some basic cleaning\n",
        "\n",
        "# remove any HTML/Markdown tags\n",
        "data[\"Instruction\"] = data[\"Instruction\"].str.replace(r'<[^<>]*>', '', regex=True)\n",
        "# remove any newline\n",
        "data[\"Instruction\"] = data[\"Instruction\"].str.replace(r'\\n',' ', regex=True)\n",
        "# remove any @user tags\n",
        "data[\"Instruction\"] = data[\"Instruction\"].str.replace(r'(?<=\\s)@[\\w]+|(?<=^)@[\\w]+', '', regex=True)\n",
        "\n",
        "# repeat same cleaning for the Response column as well\n",
        "data[\"Response\"] = data[\"Response\"].str.replace(r'<[^<>]*>', '', regex=True)\n",
        "data[\"Response\"] = data[\"Response\"].str.replace(r'\\n',' ', regex=True)\n",
        "data[\"Response\"] = data[\"Response\"].str.replace(r'(?<=\\s)@[\\w]+|(?<=^)@[\\w]+', '', regex=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.344021Z",
          "iopub.status.idle": "2024-02-22T18:59:57.344373Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.344209Z",
          "shell.execute_reply": "2024-02-22T18:59:57.344223Z"
        },
        "trusted": true,
        "id": "76IWbkb3o3ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look\n",
        "data.tail(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.34573Z",
          "iopub.status.idle": "2024-02-22T18:59:57.346045Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.345891Z",
          "shell.execute_reply": "2024-02-22T18:59:57.345904Z"
        },
        "trusted": true,
        "id": "hWwGyhNto3ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CME_dataset = []\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    instruction, response = row['Instruction'], row['Response']\n",
        "    template = (f\"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\")\n",
        "    CME_dataset.append(template)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.347311Z",
          "iopub.status.idle": "2024-02-22T18:59:57.347637Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.347477Z",
          "shell.execute_reply": "2024-02-22T18:59:57.347491Z"
        },
        "trusted": true,
        "id": "EAn1V6VKo3ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRA fine-tuning"
      ],
      "metadata": {
        "id": "FKZl0pcvo3ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 64.\n",
        "gemma_lm.backbone.enable_lora(rank=64)"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.349013Z",
          "iopub.status.idle": "2024-02-22T18:59:57.349354Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.349191Z",
          "shell.execute_reply": "2024-02-22T18:59:57.349205Z"
        },
        "trusted": true,
        "id": "oH8D2ktdo3ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the input sequence length to 512 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 512\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.349013Z",
          "iopub.status.idle": "2024-02-22T18:59:57.349354Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.349191Z",
          "shell.execute_reply": "2024-02-22T18:59:57.349205Z"
        },
        "trusted": true,
        "id": "Lyh4xBUpo3ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "gemma_lm.fit(CME_dataset, epochs=1, batch_size=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.35033Z",
          "iopub.status.idle": "2024-02-22T18:59:57.350631Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.350479Z",
          "shell.execute_reply": "2024-02-22T18:59:57.350491Z"
        },
        "trusted": true,
        "id": "RYay0VUso3ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ...and now..."
      ],
      "metadata": {
        "id": "_wsYShwXo3kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gemma_lm.generate(\"Will there be a private leaderboard shakeup?\", max_length=256))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-22T18:59:57.352096Z",
          "iopub.status.idle": "2024-02-22T18:59:57.352433Z",
          "shell.execute_reply.started": "2024-02-22T18:59:57.352277Z",
          "shell.execute_reply": "2024-02-22T18:59:57.35229Z"
        },
        "trusted": true,
        "id": "-lK1RhZ-o3kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "meh, I think that should do the trick...\n",
        "\n",
        "Now, back to that tabular dataset notebook I was working on!\n",
        "\n",
        "### Related reading\n",
        "* [Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)\n",
        "* [RAG using Llama 2, Langchain and ChromaDB](https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb) by [Gabriel Preda](https://www.kaggle.com/gpreda)\n",
        "* [Gabriel Preda \"*Developing Kaggle Notebooks*\", Packt Publishing Limited (2023)](https://www.packtpub.com/product/developing-kaggle-notebooks/9781805128519) (Chapter 10)\n",
        "* [Building A Transformer (GPT) From Scratch](https://www.kaggle.com/code/kevinbnisch/building-a-transformer-gpt-from-scratch) by [TheItCrOw](https://www.kaggle.com/kevinbnisch)\n",
        "* [Get started with Gemma using KerasNLP](https://www.kaggle.com/code/nilaychauhan/get-started-with-gemma-using-kerasnlp) by [Nilay Chauhan](https://www.kaggle.com/nilaychauhan)\n",
        "* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora) by [Nilay Chauhan](https://www.kaggle.com/nilaychauhan)"
      ],
      "metadata": {
        "id": "kxmITvNKo3kj"
      }
    }
  ]
}